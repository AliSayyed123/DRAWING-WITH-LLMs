{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":89659,"databundleVersionId":11735795,"sourceType":"competition"},{"sourceId":39911,"sourceType":"datasetVersion","datasetId":31296}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install cairosvg","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T21:17:03.516253Z","iopub.execute_input":"2025-04-13T21:17:03.516583Z","iopub.status.idle":"2025-04-13T21:17:09.615326Z","shell.execute_reply.started":"2025-04-13T21:17:03.516555Z","shell.execute_reply":"2025-04-13T21:17:09.614021Z"},"_kg_hide-output":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom wordcloud import WordCloud\nfrom tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt\nimport os\nimport random\nimport textwrap\nfrom PIL import Image\nfrom sklearn.mixture import GaussianMixture\nimport cv2\nimport time\nfrom sklearn.cluster import KMeans\nfrom skimage.segmentation import slic\nfrom skimage.color import label2rgb\nimport cairosvg\nimport io\nimport gc\nimport re\n\nINPUT_FOLDER_PATH = '/kaggle/input/flickr-image-dataset/flickr30k_images/'\n\nIMAGES_PATH = INPUT_FOLDER_PATH + 'flickr30k_images/' # contains jpg images like \"1000092795.jpg\"\nDESCRIPTIONS_PATH = INPUT_FOLDER_PATH + 'results.csv'\n\ndescriptions_df = pd.read_csv(DESCRIPTIONS_PATH, sep='|')\ndescriptions_df.dropna()\nprint(\"There are\", descriptions_df.shape[0], \"descriptions in this dataset.\")\ndescriptions_df.head()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-13T21:17:09.616500Z","iopub.execute_input":"2025-04-13T21:17:09.616782Z","iopub.status.idle":"2025-04-13T21:17:13.043831Z","shell.execute_reply.started":"2025-04-13T21:17:09.616762Z","shell.execute_reply":"2025-04-13T21:17:13.042909Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 2. Explanatory Data Analysis (EDA)","metadata":{}},{"cell_type":"code","source":"all_comments = \" \".join(descriptions_df[' comment'].astype(str).values)\n\nwordcloud = WordCloud(\n    width=800, \n    height=600, \n    background_color='white'\n).generate(all_comments)\n\nplt.figure(figsize=(10,8))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')\nplt.title(\"Word Cloud of Image Descriptions\", fontsize=16)\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T21:17:13.044688Z","iopub.execute_input":"2025-04-13T21:17:13.044983Z","iopub.status.idle":"2025-04-13T21:17:19.806763Z","shell.execute_reply.started":"2025-04-13T21:17:13.044958Z","shell.execute_reply":"2025-04-13T21:17:19.805546Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"IMAGES_PATH = '/kaggle/input/flickr-image-dataset/flickr30k_images/flickr30k_images/'\n\nsample_df = descriptions_df.sample(n=5, random_state=42)  # pick any number of rows you like\n\nfor idx, row in sample_df.iterrows():\n    image_name = row['image_name']\n    comment = row[' comment']\n\n    img_path = os.path.join(IMAGES_PATH, image_name)\n    \n    img = Image.open(img_path)\n\n    width, height = img.size\n\n    # Print out the info\n    print(f\"Image: {image_name}\")\n    print(f\"Resolution: {width} x {height}\")\n    print(f\"Comment: {comment}\")\n    print(\"-\"*50)\n    \n    plt.figure(figsize=(6,6))\n    plt.imshow(img)\n    plt.title(comment)\n    plt.axis('off')\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T21:17:19.808141Z","iopub.execute_input":"2025-04-13T21:17:19.808543Z","iopub.status.idle":"2025-04-13T21:17:20.969518Z","shell.execute_reply.started":"2025-04-13T21:17:19.808506Z","shell.execute_reply":"2025-04-13T21:17:20.968424Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"people_keywords = {\"man\",\"woman\",\"men\",\"women\",\"people\",\"person\",\"girl\",\"boy\",\"child\",\"kid\",\"couple\",\"bride\",\n                   \"groom\",\"player\",\"players\",\"children\",\"boys\",\"girls\",\"kids\"}\nanimals_keywords = {\"cat\",\"dog\",\"bird\",\"horse\",\"cow\",\"sheep\",\"goat\",\"lion\",\"tiger\",\"elephant\",\"mouse\",\"monkey\",\n                    \"gorilla\",\"bear\",\"pig\",\"duck\",\"goose\",\"whale\",\"fish\",\"cats\",\"dogs\",\"ducks\",\"birds\",\"horses\",\"animal\"}\ncolors_keywords = {\"red\",\"blue\",\"green\",\"yellow\",\"orange\",\"purple\",\"pink\",\"black\",\"brown\",\"grey\",\"gray\",\"white\"}\n\ndef classify_comment(comment: str) -> str:\n    \"\"\"Classify a comment into one of the themes based on keywords.\"\"\"\n    text = str(comment).lower()\n    if any(kw in text for kw in people_keywords):\n        return \"people\"\n    elif any(kw in text for kw in animals_keywords):\n        return \"animals\"\n    elif any(kw in text for kw in colors_keywords):\n        return \"colors\"\n    else:\n        return \"other\"\n\ndescriptions_df['theme'] = descriptions_df[' comment'].apply(classify_comment)\n\ntheme_counts = descriptions_df['theme'].value_counts()/descriptions_df.shape[0]*100\ntheme_counts = theme_counts.sort_values(ascending=False)\n\nplt.figure(figsize=(8, 6))\ntheme_counts.plot(kind='bar', rot=0, color=['#0099cc', '#99cc00', '#cc9900', '#999999'])\nplt.title(\"Theme Distribution in Descriptions\", fontsize=14)\nplt.xlabel(\"Theme\", fontsize=12)\nplt.ylabel(\"Count of Descriptions (%)\", fontsize=12)\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T21:17:20.970663Z","iopub.execute_input":"2025-04-13T21:17:20.970997Z","iopub.status.idle":"2025-04-13T21:17:21.806016Z","shell.execute_reply.started":"2025-04-13T21:17:20.970971Z","shell.execute_reply":"2025-04-13T21:17:21.804977Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**We conclude that most of the images depict people, and only a minority mentions animals or colors.**","metadata":{}},{"cell_type":"code","source":"categories = [\"people\", \"animals\", \"colors\", \"other\"]\n\nfig, axes = plt.subplots(nrows=4, ncols=4, figsize=(16, 16))\n\nfor row_idx, cat in enumerate(categories):\n    cat_df = descriptions_df[descriptions_df['theme'] == cat]\n    sample_rows = cat_df.sample(n=4, random_state=42)\n\n    for col_idx, (df_index, row) in enumerate(sample_rows.iterrows()):\n        image_path = os.path.join(IMAGES_PATH, row['image_name'])\n        \n        img = Image.open(image_path)\n\n        axes[row_idx, col_idx].imshow(img)\n        axes[row_idx, col_idx].axis('off')\n\n        wrapped_comment = textwrap.fill(row[' comment'], width=30)\n        axes[row_idx, col_idx].set_title(wrapped_comment, fontsize=10)\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T21:17:21.808524Z","iopub.execute_input":"2025-04-13T21:17:21.808812Z","iopub.status.idle":"2025-04-13T21:17:24.934748Z","shell.execute_reply.started":"2025-04-13T21:17:21.808788Z","shell.execute_reply":"2025-04-13T21:17:24.933608Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"animal_counts = {animal: 0 for animal in animals_keywords}\n\nfor comment in descriptions_df[' comment']:\n    text = str(comment).lower()\n    for animal in animals_keywords:\n        if animal in text:\n            animal_counts[animal] += 1\n\nanimal_counts_df = pd.DataFrame(animal_counts.items(), columns=[\"animal\", \"count\"])\nanimal_counts_df = animal_counts_df.sort_values(\"count\", ascending=False)\n\nplt.figure(figsize=(10, 6))\nbars = plt.bar(animal_counts_df[\"animal\"], animal_counts_df[\"count\"])\nplt.xlabel(\"Animal\")\nplt.ylabel(\"Number of Descriptions Mentioning Animal (% whole dataset)\")\nplt.title(\"Animal Mentions in Descriptions\")\nplt.xticks(rotation=45)\n\nfor bar in bars:\n    height = bar.get_height()\n    plt.text(\n        bar.get_x() + bar.get_width() / 2,   \n        height,                              \n        f\"{height / descriptions_df.shape[0] * 100:.2f}\",      \n        ha='center', va='bottom',           \n        fontsize=9\n    )\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T21:17:24.936478Z","iopub.execute_input":"2025-04-13T21:17:24.936773Z","iopub.status.idle":"2025-04-13T21:17:26.193103Z","shell.execute_reply.started":"2025-04-13T21:17:24.936747Z","shell.execute_reply":"2025-04-13T21:17:26.191956Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Most of the animals are dogs. An ML model trained on this dataset is expected to achieve low performance for images representing animals different from the \"most popular\" ones (like cats and dogs). This dataset is better for human images analysis.**","metadata":{}},{"cell_type":"markdown","source":"# 3. Seeking For Efficient Downscaling Pipelines","metadata":{}},{"cell_type":"code","source":"# Pick one random row\nsample_row = descriptions_df.sample(n=1, random_state=42).iloc[0]\nimage_name = sample_row['image_name']\nimage_path = os.path.join(IMAGES_PATH, image_name)\ncomment = sample_row[' comment']\n\nprint(f\"Randomly selected image: {image_name}\")\nprint(f\"Description: {comment}\")\n\n# Open the image\nimg = Image.open(image_path)\nimg_np = np.array(img)  # convert PIL image to a NumPy array (height x width x channels)\nplt.imshow(img_np)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T21:17:26.194093Z","iopub.execute_input":"2025-04-13T21:17:26.194386Z","iopub.status.idle":"2025-04-13T21:17:26.527201Z","shell.execute_reply.started":"2025-04-13T21:17:26.194360Z","shell.execute_reply":"2025-04-13T21:17:26.526096Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ========================\n# Helper Functions\n# ========================\n\ndef compute_entropy(image):\n    \"\"\"\n    Compute the Shannon entropy (in bits) of an image.\n    If the image is color, it is first converted to grayscale.\n    \"\"\"\n    if image.ndim == 3:\n        # Convert to grayscale using luminosity weights\n        gray = 0.299 * image[:, :, 0] + 0.587 * image[:, :, 1] + 0.114 * image[:, :, 2]\n    else:\n        gray = image\n    hist, _ = np.histogram(gray, bins=256, range=(0, 256), density=True)\n    hist = hist[hist > 0]\n    entropy = -np.sum(hist * np.log2(hist))\n    return entropy\n\ndef apply_gmm(image, n_components):\n    \"\"\"\n    Apply Gaussian Mixture on the image's pixels (preserving colors).\n    \"\"\"\n    h, w, c = image.shape\n    pixels = image.reshape(-1, c)\n    gmm = GaussianMixture(n_components=n_components, random_state=42)\n    gmm.fit(pixels)\n    labels = gmm.predict(pixels)\n    cluster_centers = gmm.means_\n    compressed_pixels = cluster_centers[labels].reshape(h, w, c)\n    compressed_pixels = np.clip(compressed_pixels, 0, 255).astype(np.uint8)\n    return compressed_pixels\n\ndef apply_opening(image, kernel):\n    \"\"\"Apply morphological opening on each channel.\"\"\"\n    opened = np.empty_like(image)\n    for i in range(image.shape[-1]):\n        opened[..., i] = cv2.morphologyEx(image[..., i], cv2.MORPH_OPEN, kernel)\n    return opened\n\ndef apply_closing(image, kernel):\n    \"\"\"Apply morphological closing on each channel.\"\"\"\n    closed = np.empty_like(image)\n    for i in range(image.shape[-1]):\n        closed[..., i] = cv2.morphologyEx(image[..., i], cv2.MORPH_CLOSE, kernel)\n    return closed\n\n# ------------------------\n# New Helper Functions\n# ------------------------\n\ndef downscale_upscale(image, downscale_factor=2, resample=Image.NEAREST):\n    \"\"\"\n    Downscale the image by 'downscale_factor' then upscale back to the original size.\n    By default, uses nearest-neighbor to preserve blocky shapes and reduce detail.\n    \"\"\"\n    pil_image = Image.fromarray(image)\n    width, height = pil_image.size\n\n    # Downscale\n    new_width = max(1, width // downscale_factor)\n    new_height = max(1, height // downscale_factor)\n    pil_small = pil_image.resize((new_width, new_height), resample=Image.BILINEAR)\n\n    # Upscale back to original size\n    pil_restored = pil_small.resize((width, height), resample=resample)\n    return np.array(pil_restored)\n\ndef downscale(image, downscale_factor=2, resample=Image.NEAREST):\n    \"\"\"\n    Downscale the image by 'downscale_factor' then upscale back to the original size.\n    By default, uses nearest-neighbor to preserve blocky shapes and reduce detail.\n    \"\"\"\n    pil_image = Image.fromarray(image)\n    width, height = pil_image.size\n\n    # Downscale\n    new_width = max(1, width // downscale_factor)\n    new_height = max(1, height // downscale_factor)\n    pil_small = pil_image.resize((new_width, new_height), resample=Image.BILINEAR)\n\n    return np.array(pil_restored)\n\ndef pil_color_quantize(image, num_colors=16):\n    \"\"\"\n    Quantize using Pillow's built-in method (median-cut or similar).\n    \"\"\"\n    pil_image = Image.fromarray(image)\n    # Convert to 'P' (palettized) with num_colors\n    quantized = pil_image.quantize(colors=num_colors, method=0, kmeans=0)\n    # Convert back to 'RGB'\n    quantized_rgb = quantized.convert(\"RGB\")\n    return np.array(quantized_rgb)\n\ndef bilateral_smoothing(image, d=9, sigma_color=75, sigma_space=75):\n    \"\"\"\n    Apply a bilateral filter to preserve edges while smoothing within regions.\n    - d: Diameter of each pixel neighborhood.\n    - sigma_color: Filter sigma in the color space.\n    - sigma_space: Filter sigma in the coordinate space.\n    \"\"\"\n    # OpenCV's bilateralFilter can process a 3-channel image in one go (BGR).\n    # But we have an RGB image, so we either convert to BGR or just apply to each channel.\n    # We'll do a simple approach: assume 'image' is in RGB and convert to BGR for bilateralFilter.\n    bgr = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n    bgr_smooth = cv2.bilateralFilter(bgr, d, sigma_color, sigma_space)\n    # Convert back to RGB\n    rgb_smooth = cv2.cvtColor(bgr_smooth, cv2.COLOR_BGR2RGB)\n    return rgb_smooth\n\ndef superpixel_simplify(image, n_segments=200, compactness=10):\n    \"\"\"\n    Partition image into superpixels (SLIC) and replace each region with its average color.\n    \"\"\"\n    # SLIC typically expects float in [0,1]\n    image_float = image.astype(np.float32) / 255.0\n    segments = slic(image_float, n_segments=n_segments, compactness=compactness, start_label=1)\n    # Re-color each segment by its average RGB\n    out = label2rgb(segments, image_float, kind='avg')\n    # Convert back to uint8\n    out = (out * 255).astype(np.uint8)\n    return out\n\nentropy_original = compute_entropy(img_np)\nprint(f\"Original image entropy: {entropy_original:.2f} bits\")\n\n# Define a kernel for morphological ops (if used)\nkernel = np.ones((3, 3), np.uint8)\n\n# ========================\n# Define Pipelines\n# ========================\n\npipelines = {\n    # ------------------------------------------------------\n    # 1. GMM pipeline\n    # ------------------------------------------------------\n    \"O+C+Gaussian 8\": lambda im: apply_gmm(apply_opening(apply_closing(im, kernel), kernel), 8),\n    \"Downscale4+O+C+Gaussian 8\": lambda im: apply_gmm(apply_opening(apply_closing(downscale_upscale(im, downscale_factor=4, resample=Image.NEAREST), kernel), kernel), 8),\n    \n    # ------------------------------------------------------\n    # 2. Downscale+Upscale\n    # ------------------------------------------------------\n    \"Downscale2+Upscale2(NN)\": lambda im: downscale_upscale(im, downscale_factor=2, resample=Image.NEAREST),\n    \"Downscale4+Upscale4(NN)\": lambda im: downscale_upscale(im, downscale_factor=4, resample=Image.NEAREST),\n    \n    # ------------------------------------------------------\n    # 3. Pillow Color Quantization\n    # ------------------------------------------------------\n    \"PIL Quantize 16\": lambda im: pil_color_quantize(im, num_colors=16),\n    \"PIL Quantize 8\":  lambda im: pil_color_quantize(im, num_colors=8),\n    \n    # ------------------------------------------------------\n    # 4. Bilateral Smoothing + Color Quant (combo)\n    # ------------------------------------------------------\n    \"Bilateral + PIL Quant16\": lambda im: pil_color_quantize(bilateral_smoothing(im), 16),\n    \n    # ------------------------------------------------------\n    # 5. Superpixel Simplify\n    # ------------------------------------------------------\n    \"Superpixel(1600)\": lambda im: superpixel_simplify(im, n_segments=1600, compactness=10),\n    \"Superpixel(800)\": lambda im: superpixel_simplify(im, n_segments=800, compactness=10),\n    \"Superpixel(400)\": lambda im: superpixel_simplify(im, n_segments=400, compactness=10),\n    \"Superpixel(200)\": lambda im: superpixel_simplify(im, n_segments=200, compactness=10),\n    \"Superpixel(100)\": lambda im: superpixel_simplify(im, n_segments=100, compactness=10),\n}\n\n# Containers for results\npipeline_names = []\nexecution_times = []\nentropies = []\n\n# ========================\n# Process Each Pipeline, Display and Save Metrics\n# ========================\n\nfor name, pipeline in pipelines.items():\n    print(f\"Processing pipeline: {name}\")\n    start_time = time.perf_counter()\n    processed = pipeline(img_np)\n    elapsed = time.perf_counter() - start_time\n    ent = compute_entropy(processed)\n    \n    pipeline_names.append(name)\n    execution_times.append(elapsed)\n    entropies.append(ent)\n    \n    # Show the processed image\n    plt.figure(figsize=(6, 6))\n    plt.imshow(processed)\n    plt.title(f\"{name}\\nTime: {elapsed:.2f} s, Entropy: {ent:.2f} bits\")\n    plt.axis(\"off\")\n    plt.show()\n\n# ========================\n# Plot Comparison of Execution Time and Entropy\n# ========================\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n\n# Bar plot for execution times\nbars1 = ax1.bar(pipeline_names, execution_times, color='#3399ff')\nax1.set_xlabel(\"Pipeline\")\nax1.set_ylabel(\"Execution Time (s)\")\nax1.set_title(\"Execution Time per Pipeline\")\nax1.tick_params(axis='x', rotation=45)\nfor bar in bars1:\n    height = bar.get_height()\n    ax1.text(bar.get_x() + bar.get_width()/2, height + 0.005, f\"{height:.2f}\", \n             ha='center', va='bottom')\n\n# Bar plot for entropies\nbars2 = ax2.bar(pipeline_names, entropies, color='#ff9933')\nax2.set_xlabel(\"Pipeline\")\nax2.set_ylabel(\"Entropy (bits)\")\nax2.set_title(\"Image Entropy per Pipeline\")\nax2.tick_params(axis='x', rotation=45)\nfor bar in bars2:\n    height = bar.get_height()\n    ax2.text(bar.get_x() + bar.get_width()/2, height + 0.5, f\"{height:.2f}\", \n             ha='center', va='bottom')\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T21:17:26.528335Z","iopub.execute_input":"2025-04-13T21:17:26.528666Z","iopub.status.idle":"2025-04-13T21:17:45.683921Z","shell.execute_reply.started":"2025-04-13T21:17:26.528624Z","shell.execute_reply":"2025-04-13T21:17:45.682416Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let's try Superpixel 800 on several images:","metadata":{}},{"cell_type":"code","source":"# Sample 16 images from the dataset\nsample_df = descriptions_df.sample(n=16, random_state=42)\n\ndef test_superpixel(seg, comp, keep_original=False, apply_OC=False):\n    # Set up a 4x4 grid for plotting\n    fig, axes = plt.subplots(4, 4, figsize=(16, 16))\n    axes = axes.ravel()\n    \n    for idx, (_, row) in enumerate(tqdm(sample_df.iterrows(), total=16)):\n        image_name = row['image_name']\n        image_path = os.path.join(IMAGES_PATH, image_name)\n        \n        # Load the image\n        im = Image.open(image_path)\n        im_np = np.array(im)\n        \n        # Drop alpha channel if present\n        if im_np.ndim == 3 and im_np.shape[-1] == 4:\n            im_np = im_np[..., :3]\n\n        if keep_original:\n            simplified_im = im_np\n        else:\n            # Apply superpixel simplification\n            simplified_im = superpixel_simplify(im_np, n_segments=seg, compactness=comp)\n            \n        if apply_OC:\n            simplified_im = apply_closing(apply_opening(simplified_im, kernel), kernel)\n        \n        # Get the description from the DataFrame. Adjust for column name with a possible leading space.\n        description = row[' comment'] if ' comment' in row else row['comment']\n        wrapped_description = textwrap.fill(description, width=30)\n        \n        # Plot the simplified image with the description as its title\n        axes[idx].imshow(simplified_im)\n        axes[idx].set_title(wrapped_description, fontsize=8)\n        axes[idx].axis('off')\n    \n    plt.tight_layout()\n    plt.show()\n\ntest_superpixel(100, 10, keep_original=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T21:17:45.684961Z","iopub.execute_input":"2025-04-13T21:17:45.685270Z","iopub.status.idle":"2025-04-13T21:17:48.756273Z","shell.execute_reply.started":"2025-04-13T21:17:45.685243Z","shell.execute_reply":"2025-04-13T21:17:48.754566Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Those were the original images. Now let's apply our filters:","metadata":{}},{"cell_type":"code","source":"test_superpixel(800, 10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T21:17:48.757698Z","iopub.execute_input":"2025-04-13T21:17:48.758191Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"What about Superpixel 1600?","metadata":{}},{"cell_type":"code","source":"test_superpixel(1600, 10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T21:18:06.741281Z","iopub.execute_input":"2025-04-13T21:18:06.741948Z","iopub.status.idle":"2025-04-13T21:18:36.760423Z","shell.execute_reply.started":"2025-04-13T21:18:06.741908Z","shell.execute_reply":"2025-04-13T21:18:36.758909Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**It appears clearly that Superpixel 800 omits some important details that are kept by Superpixel 1600**.\n\nAlthough it takes more time, I think it would be necessary to keep the second what.\n\nNow, what if we diminished compactness?","metadata":{}},{"cell_type":"code","source":"test_superpixel(1600, 5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T21:18:36.761919Z","iopub.execute_input":"2025-04-13T21:18:36.762396Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The result gets much worse, although faster. Could we rise this parameter?","metadata":{}},{"cell_type":"code","source":"test_superpixel(1600, 20)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"What about opening and closing for removing pixelization?","metadata":{}},{"cell_type":"code","source":"test_superpixel(1600, 20, apply_OC=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T21:19:34.764351Z","iopub.execute_input":"2025-04-13T21:19:34.764825Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Applying opening + closing doesn't seem to change anything here, so we will stick to the previous version.\n\nI think we found a reasonable time/quality trade.\n\n# 4. JPG to SVG Conversion\n\n*Credits for this part go to Rich Olson for [his notebook about Stable Diffusion for SVG images](https://www.kaggle.com/code/richolson/stable-diffusion-svg-scoring-metric).*\n\nThis part is inspired by Rich Olson **but was upgrated from version 15 of this notebook.** In version 15, we apply this same technique to different regions of the image separately, then combine them into a single image. Results prove to be much more efficient than for a direct application.\n\nFrom **version 16**, I decided to add background rectangles to fill the \"gaps\" created by the region separation. Indeed, separating the regions improves the level of details but also creates empty zones between each region. As shown below, it considerably improves the result:\n\n![Improvment](https://www.googleapis.com/download/storage/v1/b/kaggle-forum-message-attachments/o/inbox%2F17037041%2F54bab92da6a001a4ad3e8c722ae4dd5e%2FBackground.png?generation=1744274475823687&alt=media)\n\n**Version 19 and above** adds a solution to make the image less than 10,000 bytes for the need of the competition, but this impacts the quality.","metadata":{}},{"cell_type":"code","source":"# --------------------------------------------------------------------\n# 1) Core function for single-image vectorization\n# --------------------------------------------------------------------\ndef _bitmap_to_svg_layered_core(\n    image, \n    max_size_bytes=10000, \n    resize=True, \n    target_size=(384, 384), \n    adaptive_fill=True, \n    num_colors=None,\n    background_subdivisions=5\n):\n    \"\"\"\n    Processes a single image -> SVG (the \"core\" pipeline).\n    Also adds a grid of background rectangles covering the entire image,\n    using 'background_subdivisions' to decide how many rectangles across\n    and down.\n    \"\"\"\n    \n    # --- HELPER FUNCTIONS ---\n    def compress_hex_color(hex_color):\n        r, g, b = int(hex_color[1:3], 16), int(hex_color[3:5], 16), int(hex_color[5:7], 16)\n        # compress if possible (like #112233 -> #123)\n        if (r % 17 == 0) and (g % 17 == 0) and (b % 17 == 0):\n            return f'#{r//17:x}{g//17:x}{b//17:x}'\n        return hex_color\n\n    def ensure_rgb(np_img):\n        \"\"\"\n        Convert np_img to 3-channel RGB if it's grayscale or has an alpha channel.\n        \"\"\"\n        if np_img.ndim == 2:\n            # Grayscale => replicate channel\n            np_img = cv2.cvtColor(np_img, cv2.COLOR_GRAY2RGB)\n        elif np_img.shape[2] == 4:\n            # Drop alpha channel\n            np_img = np_img[..., :3]\n        elif np_img.shape[2] == 1:\n            # Single channel => replicate\n            np_img = np.concatenate([np_img]*3, axis=2)\n        return np_img\n\n    def extract_features_by_scale(img_np, num_colors=16):\n        img_np = ensure_rgb(img_np)\n        gray = cv2.cvtColor(img_np, cv2.COLOR_RGB2GRAY)\n        height, width = gray.shape\n        \n        # KMeans color quantization\n        pixels = img_np.reshape(-1, 3).astype(np.float32)\n        criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 100, 0.2)\n        _, labels, centers = cv2.kmeans(\n            data=pixels, K=num_colors, bestLabels=None, \n            criteria=criteria, attempts=10, flags=cv2.KMEANS_RANDOM_CENTERS\n        )\n        \n        palette = centers.astype(np.uint8)\n        quantized = palette[labels.flatten()].reshape(img_np.shape)\n        \n        # For storing final features\n        hierarchical_features = []\n        \n        # Sort color clusters by frequency\n        unique_labels, counts = np.unique(labels, return_counts=True)\n        sorted_indices = np.argsort(-counts)\n        sorted_colors = [palette[i] for i in sorted_indices]\n        \n        center_x, center_y = width / 2, height / 2\n        \n        # For each sorted color\n        for color in sorted_colors:\n            color_mask = cv2.inRange(quantized, color, color)\n            contours, _ = cv2.findContours(color_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n            contours = sorted(contours, key=cv2.contourArea, reverse=True)\n            \n            hex_color = compress_hex_color(f'#{color[0]:02x}{color[1]:02x}{color[2]:02x}')\n            \n            for contour in contours:\n                area = cv2.contourArea(contour)\n                if area < 20:\n                    continue\n                m = cv2.moments(contour)\n                if m[\"m00\"] == 0:\n                    continue\n                cx = m[\"m10\"] / m[\"m00\"]\n                cy = m[\"m01\"] / m[\"m00\"]\n                \n                # Dist from center (normalized)\n                dist_from_center = np.hypot(\n                    (cx - center_x) / width,\n                    (cy - center_y) / height\n                )\n                \n                epsilon = 0.02 * cv2.arcLength(contour, True)\n                approx = cv2.approxPolyDP(contour, epsilon, True)\n                \n                # Build points string\n                pts_str = \" \".join([f\"{pt[0][0]:.1f},{pt[0][1]:.1f}\" for pt in approx])\n                \n                # Importance\n                importance = (\n                    area * (1 - dist_from_center) * \n                    (1 / (len(approx) + 1))\n                )\n                \n                hierarchical_features.append({\n                    'points': pts_str,\n                    'color': hex_color,\n                    'area': area,\n                    'importance': importance,\n                })\n        \n        hierarchical_features.sort(key=lambda x: x['importance'], reverse=True)\n        return hierarchical_features\n\n    def simplify_polygon(points_str, simplification_level):\n        \"\"\"\n        Different simplification levels:\n            0 -> no change\n            1 -> round coords to .1f\n            2 -> round coords to integer\n            3 -> keep ~half the points (or to integer if too few)\n        \"\"\"\n        if simplification_level == 0:\n            return points_str\n        \n        coords = points_str.strip().split()\n        if simplification_level == 1:\n            return \" \".join([\n                f\"{float(c.split(',')[0]):.1f},{float(c.split(',')[1]):.1f}\"\n                for c in coords\n            ])\n        elif simplification_level == 2:\n            return \" \".join([\n                f\"{round(float(c.split(',')[0]))},{round(float(c.split(',')[1]))}\"\n                for c in coords\n            ])\n        elif simplification_level == 3:\n            if len(coords) <= 4:\n                # Just round to int\n                return \" \".join([\n                    f\"{round(float(c.split(',')[0]))},{round(float(c.split(',')[1]))}\"\n                    for c in coords\n                ])\n            else:\n                # Keep half the points\n                step = min(2, len(coords) // 3)\n                reduced = [coords[i] for i in range(0, len(coords), step)]\n                if len(reduced) < 3:\n                    reduced = coords[:3]\n                if coords[-1] not in reduced:\n                    reduced.append(coords[-1])\n                return \" \".join([\n                    f\"{round(float(c.split(',')[0]))},{round(float(c.split(',')[1]))}\"\n                    for c in reduced\n                ])\n        return points_str\n\n    def average_color_in_subregion(np_img, x_start, y_start, w, h):\n        \"\"\"\n        Return a 3-channel average (R,G,B), dropping alpha or\n        converting grayscale if necessary.\n        \"\"\"\n        sub = np_img[y_start:y_start+h, x_start:x_start+w]\n\n        # ensure sub is NxMx3\n        sub = ensure_rgb(sub)\n\n        # shape is (h, w, 3)\n        mean_vals = sub.mean(axis=(0,1))\n        # mean_vals is shape (3,) => (R,G,B)\n        return mean_vals\n    \n    # ---------------- MAIN PIPELINE ---------------\n    \n    # 1) Possibly pick num_colors if not set\n    if num_colors is None:\n        if resize:\n            pixel_count = target_size[0] * target_size[1]\n        else:\n            pixel_count = image.size[0] * image.size[1]\n        if pixel_count < 65536:\n            num_colors = 8\n        elif pixel_count < 262144:\n            num_colors = 12\n        else:\n            num_colors = 16\n\n    # 2) Resize if needed\n    if resize:\n        original_size = image.size\n        image = image.resize(target_size, Image.LANCZOS)\n    else:\n        original_size = image.size\n\n    # Convert to np\n    img_np = np.array(image)\n    # ensure 3-channel RGB\n    img_np = ensure_rgb(img_np)\n\n    height, width = img_np.shape[:2]\n\n    # Build SVG header\n    orig_w, orig_h = original_size\n    svg_header = (\n        f'<svg xmlns=\"http://www.w3.org/2000/svg\" '\n        f'width=\"{orig_w}\" height=\"{orig_h}\" '\n        f'viewBox=\"0 0 {width} {height}\">\\n'\n    )\n\n    # NxN background squares\n    n = background_subdivisions\n    cell_w = width / n\n    cell_h = height / n\n    bg_rects = []\n\n    for row in range(n):\n        for col in range(n):\n            x0 = int(col * cell_w)\n            y0 = int(row * cell_h)\n            if col == n - 1:\n                w_sub = width - x0\n            else:\n                w_sub = int(cell_w)\n            if row == n - 1:\n                h_sub = height - y0\n            else:\n                h_sub = int(cell_h)\n\n            avg_col = average_color_in_subregion(img_np, x0, y0, w_sub, h_sub)\n            # avg_col is (r,g,b)\n            r, g, b = avg_col\n            hex_col = compress_hex_color(f'#{int(r):02x}{int(g):02x}{int(b):02x}')\n            bg_rects.append(\n                f'<rect x=\"{x0}\" y=\"{y0}\" width=\"{w_sub}\" height=\"{h_sub}\" fill=\"{hex_col}\"/>\\n'\n            )\n\n    svg_base = svg_header + \"\".join(bg_rects)\n    svg_footer = '</svg>'\n    base_size = len((svg_base + svg_footer).encode('utf-8'))\n\n    # Extract features\n    features = extract_features_by_scale(img_np, num_colors=num_colors)\n\n    # If not adaptive fill, just add polygons until out of space\n    if not adaptive_fill:\n        svg = svg_base\n        for feat in features:\n            poly_tag = f'<polygon points=\"{feat[\"points\"]}\" fill=\"{feat[\"color\"]}\" />\\n'\n            trial_size = len((svg + poly_tag + svg_footer).encode('utf-8'))\n            if trial_size > max_size_bytes:\n                break\n            svg += poly_tag\n        svg += svg_footer\n        return svg\n\n    # Otherwise do adaptive fill\n    def size_of_polygon(points_str, fill_str):\n        return len(f'<polygon points=\"{points_str}\" fill=\"{fill_str}\" />\\n'.encode('utf-8'))\n\n    def polygon_tag(points_str, fill_str):\n        return f'<polygon points=\"{points_str}\" fill=\"{fill_str}\" />\\n'\n\n    # Precompute size at each simplification level\n    feature_sizes = []\n    for feat in features:\n        pts0 = feat[\"points\"]\n        c = feat[\"color\"]\n\n        s0 = size_of_polygon(pts0, c)\n        pts1 = simplify_polygon(pts0, 1)\n        s1 = size_of_polygon(pts1, c)\n        pts2 = simplify_polygon(pts0, 2)\n        s2 = size_of_polygon(pts2, c)\n        pts3 = simplify_polygon(pts0, 3)\n        s3 = size_of_polygon(pts3, c)\n\n        feature_sizes.append({\n            'original': s0,\n            'level1': s1,\n            'level2': s2,\n            'level3': s3\n        })\n\n    svg = svg_base\n    bytes_used = base_size\n    added_features = set()\n\n    # Pass 1: original polygons\n    for i, feat in enumerate(features):\n        s0 = feature_sizes[i]['original']\n        if bytes_used + s0 <= max_size_bytes:\n            svg += polygon_tag(feat[\"points\"], feat[\"color\"])\n            bytes_used += s0\n            added_features.add(i)\n\n    # Pass 2: simplified polygons\n    for level in range(1, 4):\n        key = f'level{level}'\n        for i, feat in enumerate(features):\n            if i in added_features:\n                continue\n            sz = feature_sizes[i][key]\n            if bytes_used + sz <= max_size_bytes:\n                spoints = simplify_polygon(feat[\"points\"], level)\n                svg += polygon_tag(spoints, feat[\"color\"])\n                bytes_used += sz\n                added_features.add(i)\n\n    svg += svg_footer\n\n    final_size = len(svg.encode('utf-8'))\n    if final_size > max_size_bytes:\n        # fallback\n        fallback_svg = (\n            svg_header\n            + f'<rect width=\"{width}\" height=\"{height}\" fill=\"#fff\"/>\\n'\n            + '</svg>'\n        )\n        return fallback_svg\n\n    return svg\n\n\n# --------------------------------------------------------------------\n# 2) Extended function with iteration_passes (and forced pass2=>sep=1)\n# --------------------------------------------------------------------\ndef bitmap_to_svg_layered(\n    image,\n    max_size_bytes=10000,\n    resize=True,\n    target_size=(384, 384),\n    adaptive_fill=True,\n    num_colors=None,\n    separation_factor=2,\n    background_subdivisions=5,\n    iteration_passes=1\n):\n    \"\"\"\n    Extended version that:\n      1) Can tile the image (separation_factor>1) on the FIRST pass only,\n      2) Adds NxN background rectangles,\n      3) And can apply multiple passes (iteration_passes),\n         re-rasterizing the last pass's SVG result for the next pass.\n      4) Forces separation_factor=1 for the second+ pass to avoid repeating tiling.\n    \"\"\"\n\n    def svg_to_png(svg_str):\n        # Convert SVG -> PNG in memory\n        png_data = cairosvg.svg2png(bytestring=svg_str.encode('utf-8'))\n        # Convert PNG bytes -> PIL Image\n        return Image.open(io.BytesIO(png_data))\n    \n    current_image = image\n\n    for pass_idx in range(iteration_passes):\n        # Force separation_factor=1 on second+ passes\n        if pass_idx == 0:\n            local_sep_factor = separation_factor\n        else:\n            local_sep_factor = 1\n\n        # If local_sep_factor<=1 => single pass\n        if local_sep_factor <= 1:\n            final_svg = _bitmap_to_svg_layered_core(\n                current_image,\n                max_size_bytes=max_size_bytes,\n                resize=resize,\n                target_size=target_size,\n                adaptive_fill=adaptive_fill,\n                num_colors=num_colors,\n                background_subdivisions=background_subdivisions\n            )\n        else:\n            # Tiling approach (first pass only)\n            orig_w, orig_h = current_image.size\n            tile_w = orig_w // local_sep_factor\n            tile_h = orig_h // local_sep_factor\n\n            final_svg_w = orig_w * local_sep_factor\n            final_svg_h = orig_h * local_sep_factor\n\n            final_svg_header = (\n                f'<svg xmlns=\"http://www.w3.org/2000/svg\" '\n                f'width=\"{final_svg_w}\" height=\"{final_svg_h}\" '\n                f'viewBox=\"0 0 {final_svg_w} {final_svg_h}\">\\n'\n            )\n            # build NxN background squares for entire mosaic\n            # (just like in _bitmap_to_svg_layered_core)\n            big_image = current_image.resize((final_svg_w, final_svg_h), Image.LANCZOS)\n            big_np = np.array(big_image)\n\n            # local helper\n            def compress_hex_color(hex_color):\n                r, g, b = int(hex_color[1:3], 16), int(hex_color[3:5], 16), int(hex_color[5:7], 16)\n                if (r % 17 == 0) and (g % 17 == 0) and (b % 17 == 0):\n                    return f'#{r//17:x}{g//17:x}{b//17:x}'\n                return hex_color\n\n            def ensure_rgb(np_img):\n                if np_img.ndim == 2:\n                    np_img = cv2.cvtColor(np_img, cv2.COLOR_GRAY2RGB)\n                elif np_img.shape[2] == 4:\n                    np_img = np_img[..., :3]\n                elif np_img.shape[2] == 1:\n                    np_img = np.concatenate([np_img]*3, axis=2)\n                return np_img\n\n            big_np = ensure_rgb(big_np)\n            n = background_subdivisions\n            cell_w = final_svg_w / n\n            cell_h = final_svg_h / n\n\n            def average_color_in_subregion(np_img, x_start, y_start, w, h):\n                sub = np_img[y_start:y_start+h, x_start:x_start+w]\n                sub = ensure_rgb(sub)\n                mean_vals = sub.mean(axis=(0,1))\n                return mean_vals\n\n            bg_rects = []\n            for row in range(n):\n                for col in range(n):\n                    x0 = int(col * cell_w)\n                    y0 = int(row * cell_h)\n                    if col == n - 1:\n                        w_sub = final_svg_w - x0\n                    else:\n                        w_sub = int(cell_w)\n                    if row == n - 1:\n                        h_sub = final_svg_h - y0\n                    else:\n                        h_sub = int(cell_h)\n\n                    avg_col = average_color_in_subregion(big_np, x0, y0, w_sub, h_sub)\n                    r, g, b = avg_col\n                    color_hex = compress_hex_color(f'#{int(r):02x}{int(g):02x}{int(b):02x}')\n                    bg_rects.append(\n                        f'<rect x=\"{x0}\" y=\"{y0}\" width=\"{w_sub}\" height=\"{h_sub}\" fill=\"{color_hex}\"/>\\n'\n                    )\n\n            final_svg_footer = '</svg>'\n            combined_polygons = [final_svg_header] + bg_rects\n\n            # We'll parse polygons from each tile’s individual SVG\n            polygon_pattern = re.compile(\n                r'<polygon\\s+points=\"([^\"]+)\"\\s+fill=\"([^\"]+)\"\\s*/>'\n            )\n\n            # For each tile\n            for row in range(local_sep_factor):\n                for col in range(local_sep_factor):\n                    left = col * tile_w\n                    top = row * tile_h\n                    right = left + tile_w\n                    bottom = top + tile_h\n                    tile = current_image.crop((left, top, right, bottom))\n\n                    tile_upscaled = tile.resize((orig_w, orig_h), Image.LANCZOS)\n\n                    # Single pass on each tile\n                    tile_svg = _bitmap_to_svg_layered_core(\n                        tile_upscaled,\n                        max_size_bytes=max_size_bytes,\n                        resize=False,\n                        target_size=target_size,\n                        adaptive_fill=adaptive_fill,\n                        num_colors=num_colors,\n                        background_subdivisions=background_subdivisions\n                    )\n\n                    # Extract polygons\n                    matches = polygon_pattern.findall(tile_svg)\n                    x_offset = col * orig_w\n                    y_offset = row * orig_h\n\n                    for (points_str, fill_color) in matches:\n                        pts = points_str.strip().split()\n                        new_pts = []\n                        for p in pts:\n                            xs, ys = p.split(',')\n                            x = float(xs) + x_offset\n                            y = float(ys) + y_offset\n                            new_pts.append(f\"{x:.1f},{y:.1f}\")\n                        offset_points_str = \" \".join(new_pts)\n                        combined_polygons.append(\n                            f'<polygon points=\"{offset_points_str}\" fill=\"{fill_color}\" />\\n'\n                        )\n\n            combined_polygons.append(final_svg_footer)\n            final_svg = \"\".join(combined_polygons)\n\n        # Print size info\n        svg_size = len(final_svg.encode('utf-8'))\n        print(f\"[Pass {pass_idx+1}/{iteration_passes}] separation_factor={local_sep_factor}, SVG size = {svg_size} bytes\")\n\n        # If not the last pass, rasterize for next iteration\n        if pass_idx < iteration_passes - 1:\n            rebitmap = svg_to_png(final_svg)\n            current_image = rebitmap\n\n    # Return final SVG\n    return final_svg","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now let's apply the full pipeline JPG/SVG:","metadata":{}},{"cell_type":"code","source":"def test_superpixel_to_svg(sample_df, seg, comp, keep_original=False, apply_OC=False, max_images=16,\n                           iteration_passes=1, separation_factor=2):\n    \"\"\"\n    Process up to 'max_images' from 'sample_df' in a 4x4 grid:\n      1) Superpixel simplification (unless keep_original=True).\n      2) Optional morphological Opening+Closing.\n      3) Convert final result into an SVG using 'bitmap_to_svg_layered'.\n      4) Render the SVG as a PNG and display in a subplot, titled by the sample's description.\n    \n    Returns:\n        dict: { image_name: svg_string } for each processed image\n    \"\"\"\n    \n    # We'll process only 'max_images' from the sample_df (commonly 16 for a 4x4 grid)\n    df_subset = sample_df.head(max_images)\n\n    # Set up a 4x4 grid for plotting\n    rows = 4\n    cols = 4\n    fig, axes = plt.subplots(rows, cols, figsize=(16, 16))\n    axes = axes.ravel()\n\n    svgs = {}  # Will store {image_name: svg_code}\n\n    for idx, (_, row) in enumerate(tqdm(df_subset.iterrows(), total=len(df_subset))):\n        image_name = row['image_name']\n        image_path = os.path.join(IMAGES_PATH, image_name)\n\n        # Load the image\n        im = Image.open(image_path)\n        im_np = np.array(im)\n\n        # Drop alpha channel if present\n        if im_np.ndim == 3 and im_np.shape[-1] == 4:\n            im_np = im_np[..., :3]\n\n        # 1) Superpixel simplification\n        if not keep_original:\n            simplified_im = superpixel_simplify(im_np, n_segments=seg, compactness=comp)\n        else:\n            simplified_im = im_np\n\n        # 2) Optional morphological Opening + Closing\n        if apply_OC:\n            opened = apply_opening(simplified_im, kernel)\n            closed = apply_closing(opened, kernel)\n            final_im = closed\n        else:\n            final_im = simplified_im\n\n        # 3) Convert final preprocessed result to SVG\n        final_pil = Image.fromarray(final_im)\n        svg_code = bitmap_to_svg_layered(final_pil, max_size_bytes=10000, resize=False,\n                                         iteration_passes=iteration_passes, separation_factor=separation_factor)\n        svgs[image_name] = svg_code\n\n        # 4) Render the SVG in the subplot\n        #    Convert the SVG string to a PNG in memory and then load it into Pillow.\n        if idx < rows*cols:  # Ensure we have a subplot slot\n            # Convert SVG string to PNG bytes\n            png_data = cairosvg.svg2png(bytestring=svg_code.encode('utf-8'))\n            # Load into PIL\n            png_image = Image.open(io.BytesIO(png_data))\n\n            # Get the description\n            description = row[' comment'] if ' comment' in row else row['comment']\n            wrapped_description = textwrap.fill(description, width=30)\n\n            axes[idx].imshow(png_image)\n            axes[idx].set_title(wrapped_description, fontsize=8)\n            axes[idx].axis('off')\n\n    # Adjust layout and show\n    plt.tight_layout()\n    plt.show()\n\n    # Return the dictionary of SVGs\n    return svgs","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"svgs = test_superpixel_to_svg(sample_df, seg=1600, comp=20, apply_OC=False)\nfor img_name, svg_code in svgs.items():\n    print(f\"{img_name} -> {len(svg_code.encode('utf-8'))} bytes in SVG\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We clearly see the effect of the separation factor, but the result is much better than when we tried without it (see version 14 of this notebook). However, it takes a considerable time to execute and is too heavy for the competition requirements. We have the same problem for seg=800 in superpixel.\n\nWhat if we applied twice the same pipeline to smoothen the details? I wrote the code so that the separation factor would be 1 for the second image.","metadata":{}},{"cell_type":"code","source":"svgs = test_superpixel_to_svg(sample_df, seg=1600, comp=20, apply_OC=False, iteration_passes=2)\nfor img_name, svg_code in svgs.items():\n    print(f\"{img_name} -> {len(svg_code.encode('utf-8'))} bytes in SVG\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The new images are successfully below 10,000 bytes, but we could question the method efficiency.\n\nWhat if we applied SVG conversion from original images directly?","metadata":{}},{"cell_type":"code","source":"svgs_from_originals = test_superpixel_to_svg(sample_df, seg=1600, comp=20, apply_OC=False, keep_original=True)\nfor img_name, svg_code in svgs_from_originals.items():\n    print(f\"{img_name} -> {len(svg_code.encode('utf-8'))} bytes in SVG\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}